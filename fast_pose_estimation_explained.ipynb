{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530592a8",
   "metadata": {},
   "source": [
    "# Lightweight Single Person Pose Estimation with Tensorflow\n",
    "### Using Pre Trained MoveNet model and Tensorflow\n",
    "### MoveNet singlepose, pose Estimation \n",
    "### Lightweight pose estimation\n",
    "### Useing movenet tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd495c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp38-cp38-win_amd64.whl (438.0 MB)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (3.17.3)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (1.38.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (1.21.0)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9 MB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.25.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.5)\n",
      "Requirement already satisfied: oauthlib<3.0.0,>=2.1.0 in c:\\users\\isharac\\appdata\\roaming\\python\\python38\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (2.1.0)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=21fd9fece474b053a1a350eb9eafc32c7a1106206029dfa98c310899956b6001\n",
      "  Stored in directory: c:\\users\\isharac\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.7.0\n",
      "    Uninstalling tensorboard-2.7.0:\n",
      "      Successfully uninstalled tensorboard-2.7.0\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-2.0 gast-0.5.3 google-pasta-0.2.0 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 opt-einsum-3.3.0 tensorboard-2.8.0 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
     ]
    }
   ],
   "source": [
    "#tensorflow is an dependency\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eaa6a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8769617",
   "metadata": {},
   "source": [
    "### download pretrained model from tensorflow hub\n",
    "https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/3\n",
    "\n",
    "### copy model to project path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b4c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model from file\n",
    "interpreter = tf.lite.Interpreter(model_path='lite-model_movenet_singlepose_lightning_3.tflite')\n",
    "#pre allocateing tensors for model\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c8bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test code\n",
    "#capture webcam for detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()    \n",
    "    cv2.imshow('Movement', frame)    \n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f1dfb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n",
      "(1, 480, 640, 3)\n",
      "(1, 192, 192, 3)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2135e49abb0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAANcElEQVR4nO3db6hk9X3H8fenWq1NU6L1D+KfuIoJrEVv2sU+EMXUJppQslowXSlhaaWroNDSPqim0EghENpYn7QalIhbSNbYGqsPbKJIiRSa6m7cGv9tXP9Er7usGw2NrSFh128fzLlkst7prnNm7sze3/sFlznnN+fM+f4c/HDOmdn5pqqQ1K5fmHUBkmbLEJAaZwhIjTMEpMYZAlLjDAGpcVMLgSSXJdmRZGeSG6Z1HEn9ZBrfE0hyBPA94GPAIvA4cFVVPTPxg0nqZVpnAucDO6vqxar6KXA3sH5Kx5LUw5FTet1TgFeH1heB3xq1cRK/tihN3w+q6oQDB6cVAllm7Of+R0+yCdg0peNLerfvLzc4rRBYBE4bWj8V2DW8QVXdDtwOnglIszStewKPA2cnWZPkKGAD8MCUjiWph6mcCVTVviTXA98EjgDurKqnp3EsSf1M5SPC91yElwPSSthWVesOHPQbg1LjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxhoDUOENAatzYIZDktCT/luTZJE8n+ZNu/KYkryXZ3v19cnLlSpq0Pj85vg/486r6TpL3A9uSPNw9d0tVffFQX2jt2rVs2bKlRymSDua8885bdnzsEKiq3cDubvmtJM8y6EH4nh1zzDGce+6545YiqYeJ3BNIcgbwEeA/u6HrkzyZ5M4kx47YZ1OSrUm27t27dxJlSBpD7xBI8ivAvcCfVtWPgNuAs4AFBmcKNy+3X1XdXlXrqmrdCSe8q1GqpBXSKwSS/CKDAPhKVX0doKr2VNX+qnoHuAM4v3+Zkqalz6cDAb4MPFtVfzc0fvLQZlcAT41fnqRp6/PpwAXAZ4DvJtnejX0WuCrJAlDAy8A1PY4hacr6fDrw70CWeerB8cuRtNL8xqDUOENAapwhIDXOEJAaZwhIjTMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNa7PD42S5GXgLWA/sK+q1iU5DvgacAaDHxr9dFX9sF+ZkqZlEmcCH62qhapa163fADxSVWcDj3TrkubUNC4H1gObu+XNwOVTOIakCekbAgU8lGRbkk3d2Elds9KlpqUn9jyGpCnqdU8AuKCqdiU5EXg4yXOHumMXGpsATj/99J5lSBpXrzOBqtrVPb4O3Meg7+CepVZk3ePrI/a1Iak0B/r0InxfkvcvLQMfZ9B38AFgY7fZRuD+vkVKmp4+lwMnAfcN+pJyJPDVqvpGkseBe5JcDbwCXNm/TEnT0qcX4YvAecuMvwFc0qcoSSvHbwxKjTMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuPG/o3BJB9m0HNwyZnAXwEfAP4Y2NuNf7aqHhz3OJKmq88Pje4AFgCSHAG8xqD3wB8Ct1TVFydRoKTpmtTlwCXAC1X1/Qm9nqQVMqkQ2ABsGVq/PsmTSe5McuyEjiFpCnqHQJKjgE8B/9QN3QacxeBSYTdw84j9NiXZmmTr3r17l9tE0gqYxJnAJ4DvVNUegKraU1X7q+od4A4G/QnfxV6E0nyYRAhcxdClwFIz0s4VDPoTSppTvVqTJ/ll4GPANUPDf5NkASjg5QOekzRneoVAVb0N/NoBY5/pVZGkFeU3BqXGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxhoDUOENAapwhIDXOEJAaZwhIjTMEpMYZAlLjDAGpcQcNga6ByOtJnhoaOy7Jw0me7x6PHXruxiQ7k+xIcum0Cpc0GYdyJnAXcNkBYzcAj1TV2cAj3TpJ1jLoRnROt8+tXZ9CSXPqoCFQVY8Cbx4wvB7Y3C1vBi4fGr+7qn5SVS8BOxnRfETSfBj3nsBJVbUboHs8sRs/BXh1aLvFbkzSnJr0jcEsM1bLbmgvQmkujBsCe5bajXWPr3fji8BpQ9udCuxa7gXsRSjNh3FD4AFgY7e8Ebh/aHxDkqOTrAHOBh7rV6KkaTpoG7IkW4CLgeOTLAKfA74A3JPkauAV4EqAqno6yT3AM8A+4Lqq2j+l2iVNwEFDoKquGvHUJSO2/zzw+T5FSVo5fmNQapwhIDXOEJAaZwhIjTMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNG7ch6d8meS7Jk0nuS/KBbvyMJD9Osr37+9IUa5c0AeM2JH0Y+PWqOhf4HnDj0HMvVNVC93ftZMqUNC1jNSStqoeqal+3+m0GnYYkHYYmcU/gj4B/HVpfk+SJJN9KcuGonexFKM2HXiGQ5C8ZdBr6Sje0Gzi9qj4C/Bnw1SS/uty+9iKU5sPYIZBkI/C7wB9UVQFU1U+q6o1ueRvwAvChSRQqaTrGCoEklwF/AXyqqt4eGj8hyRHd8pkMGpK+OIlCJU3HuA1JbwSOBh5OAvDt7pOAi4C/TrIP2A9cW1VvLvvCkubCuA1Jvzxi23uBe/sWJWnl+I1BqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxhoDUOENAapwhIDXOEJAaZwhIjTMEpMYZAlLjDAGpcYaA1DhDQGrcuL0Ib0ry2lDPwU8OPXdjkp1JdiS5dFqFS5qMcXsRAtwy1HPwQYAka4ENwDndPrcu/QS5pPk0Vi/C/8d64O6uCclLwE7g/B71SZqyPvcEru9ak9+Z5Nhu7BTg1aFtFrsxSXNq3BC4DTgLWGDQf/DmbjzLbFvLvYANSaX5MFYIVNWeqtpfVe8Ad/CzU/5F4LShTU8Fdo14DRuSrkJvAXcCP2RE+mvujNuL8OSh1SuApU8OHgA2JDk6yRoGvQgf61eiDif/w+BOsiFw+Bi3F+HFSRYYvM8vA9cAVNXTSe4BnmHQsvy6qto/lco1l04GHp11EXpP0nUVn6l169bV1q1bZ12GtKol2VZV6w4c9xuDUuMMAa2IHcATsy5CyzroPQFpEj4IvDPrIrQsQ0Ar4pdmXYBG8nJAapwhIDXOEJAaZwhIjTMEpMYZAg3YDHx61kVobvkRYQMuBM6cdRGaW4ZAA04B/MfaGsUQaMDR3Z+0HO8JSI2bizOBt99+m+3bt8+6DKlJc/F7AklmX4S0+vl7ApLezRCQGmcISI0btxfh14b6EL6cZHs3fkaSHw8996Up1i5pAg7l04G7gL8H/nFpoKp+f2k5yc3Afw9t/0JVLUyoPklTdtAQqKpHk5yx3HNJwuBr6b894bokrZC+9wQuBPZU1fNDY2uSPJHkW0ku7Pn6kqas75eFrgK2DK3vBk6vqjeS/CbwL0nOqaofHbhjkk3App7Hl9TT2GcCSY4Efg/42tJY15L8jW55G/AC8KHl9h/uRThuDZL663M58DvAc1W1uDSQ5IQkR3TLZzLoRfhivxIlTdOhfES4BfgP4MNJFpNc3T21gZ+/FAC4CHgyyX8B/wxcW1VvTrJgSZPlvx2Q2uG/HZD0boaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNm4tehMAPgP/tHlez41ndc1zt84PDe44fXG5wLn5PACDJ1tX+U2OrfY6rfX6wOufo5YDUOENAatw8hcDtsy5gBaz2Oa72+cEqnOPc3BOQNBvzdCYgaQZmHgJJLkuyI8nOJDfMup5J6bo1f7frzry1GzsuycNJnu8ej511ne/FiA7VI+eU5Mbufd2R5NLZVH3oRszvpiSvDXXa/uTQc4fV/EaZaQh0jUr+AfgEsBa4KsnaWdY0YR+tqoWhj5RuAB6pqrOBR7r1w8ldwGUHjC07p+593ACc0+1z61Jjmjl2F++eH8At3fu4UFUPwmE7v2XN+kzgfGBnVb1YVT8F7gbWz7imaVoPbO6WNwOXz66U966qHgUObCYzak7rgbu71nQvATsZvN9za8T8Rjns5jfKrEPgFODVofXFbmw1KOChJNu65qsAJ1XVboDu8cSZVTc5o+a0mt7b65M82V0uLF3urJr5zToEsszYavm44oKq+g0GlzrXJblo1gWtsNXy3t4GnAUsMOi6fXM3vlrmN/MQWAROG1o/Fdg1o1omqqp2dY+vA/cxOFXck+RkgO7x9dlVODGj5rQq3tuq2lNV+6vqHeAOfnbKvyrmB7MPgceBs5OsSXIUgxstD8y4pt6SvC/J+5eWgY8DTzGY28Zus43A/bOpcKJGzekBYEOSo5OsYdCh+rEZ1NfLUsB1rmDwPsIqmR/M+F8RVtW+JNcD3wSOAO6sqqdnWdOEnATclwQG/42/WlXfSPI4cE/X2fkV4MoZ1viedR2qLwaOT7IIfA74AsvMqaqeTnIP8AywD7iuqvbPpPBDNGJ+FydZYHCq/zJwDRye8xvFbwxKjZv15YCkGTMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGvd/prPqvUxGsAkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test code\n",
    "#image reshapeing breakdown\n",
    "# Getting copy of frame\n",
    "image = frame.copy()\n",
    "# the shape of frame\n",
    "print(image.shape)\n",
    "\n",
    "#encapulate frame in another array\n",
    "image_enc = np.expand_dims(image, axis=0)\n",
    "#shape of encapsulated image\n",
    "print(image_enc.shape)\n",
    "\n",
    "#reshape encapsulated image with resize pad\n",
    "reshaped_Image = tf.image.resize_with_pad(image_enc, 192,192)\n",
    "#reshaped image shape\n",
    "print(reshaped_Image.shape)\n",
    "\n",
    "#reshaped image is tensor \n",
    "print(type(reshaped_Image))\n",
    "\n",
    "#converting tensor back to intiger by type casting\n",
    "new_image = tf.cast(np.squeeze(reshaped_Image),dtype=tf.float32)\n",
    "plt.imshow(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bd227ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'serving_default_input:0', 'index': 0, 'shape': array([  1, 192, 192,   3]), 'shape_signature': array([  1, 192, 192,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'StatefulPartitionedCall:0',\n",
       "  'index': 312,\n",
       "  'shape': array([ 1,  1, 17,  3]),\n",
       "  'shape_signature': array([ 1,  1, 17,  3]),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test code\n",
    "#get input detailed from model\n",
    "print(interpreter.get_input_details())\n",
    "#get model output detailes\n",
    "interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c51c06d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test code\n",
    "#get index value from output detailes\n",
    "output_details[0]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "beb44765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.7537007  0.5271887  0.39369392]\n",
      "   [0.71971655 0.57373786 0.5655149 ]\n",
      "   [0.71553063 0.48083878 0.6121105 ]\n",
      "   [0.7375107  0.62939894 0.51363486]\n",
      "   [0.7237599  0.42537516 0.51438344]\n",
      "   [0.8354615  0.68290675 0.24814764]\n",
      "   [0.837286   0.39169618 0.20537058]\n",
      "   [0.83250463 0.73406446 0.07610495]\n",
      "   [0.6823663  0.37359247 0.05345926]\n",
      "   [0.7455396  0.61491895 0.10641114]\n",
      "   [0.7368555  0.43218645 0.05857129]\n",
      "   [0.70327634 0.6838245  0.06079926]\n",
      "   [0.6573832  0.39990458 0.07501464]\n",
      "   [0.6772587  0.7100462  0.06191339]\n",
      "   [0.69917095 0.3939227  0.02953783]\n",
      "   [0.7288749  0.63714004 0.04579248]\n",
      "   [0.72633004 0.45935428 0.03352031]]]]\n",
      "[[0.7537007  0.5271887  0.39369392]\n",
      " [0.71971655 0.57373786 0.5655149 ]\n",
      " [0.71553063 0.48083878 0.6121105 ]\n",
      " [0.7375107  0.62939894 0.51363486]\n",
      " [0.7237599  0.42537516 0.51438344]\n",
      " [0.8354615  0.68290675 0.24814764]\n",
      " [0.837286   0.39169618 0.20537058]\n",
      " [0.83250463 0.73406446 0.07610495]\n",
      " [0.6823663  0.37359247 0.05345926]\n",
      " [0.7455396  0.61491895 0.10641114]\n",
      " [0.7368555  0.43218645 0.05857129]\n",
      " [0.70327634 0.6838245  0.06079926]\n",
      " [0.6573832  0.39990458 0.07501464]\n",
      " [0.6772587  0.7100462  0.06191339]\n",
      " [0.69917095 0.3939227  0.02953783]\n",
      " [0.7288749  0.63714004 0.04579248]\n",
      " [0.72633004 0.45935428 0.03352031]]\n",
      "[[0.7537007  0.5271887  0.39369392]\n",
      " [0.71971655 0.57373786 0.5655149 ]\n",
      " [0.71553063 0.48083878 0.6121105 ]\n",
      " [0.7375107  0.62939894 0.51363486]\n",
      " [0.7237599  0.42537516 0.51438344]\n",
      " [0.8354615  0.68290675 0.24814764]\n",
      " [0.837286   0.39169618 0.20537058]\n",
      " [0.83250463 0.73406446 0.07610495]\n",
      " [0.6823663  0.37359247 0.05345926]\n",
      " [0.7455396  0.61491895 0.10641114]\n",
      " [0.7368555  0.43218645 0.05857129]\n",
      " [0.70327634 0.6838245  0.06079926]\n",
      " [0.6573832  0.39990458 0.07501464]\n",
      " [0.6772587  0.7100462  0.06191339]\n",
      " [0.69917095 0.3939227  0.02953783]\n",
      " [0.7288749  0.63714004 0.04579248]\n",
      " [0.72633004 0.45935428 0.03352031]]\n"
     ]
    }
   ],
   "source": [
    "#test code\n",
    "#set image as input to model, image is wrapped in numpy array\n",
    "interpreter.set_tensor(input_details[0]['index'], np.array(reshaped_Image))\n",
    "#make prediction with model\n",
    "interpreter.invoke()\n",
    "#getting reults from the model\n",
    "keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(keypoints_with_scores)\n",
    "\n",
    "#can use np.squeeze to extract array witihing array\n",
    "print(np.squeeze(keypoints_with_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed1d36",
   "metadata": {},
   "source": [
    "### Output landmarks array order according to the documentation\n",
    " [nose, left eye, right eye, left ear, right ear, left shoulder, right shoulder, left elbow, right elbow, left wrist, right wrist, left hip, right hip, left knee, right knee, left ankle, right ankle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "643ffc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71971655, 0.57373786, 0.5655149 ], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test code\n",
    "#get left eye landmark\n",
    "leftEye = keypoints_with_scores[0][0][1]\n",
    "leftEye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1176a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[345.46394348 367.19223022]\n",
      "[345 367]\n"
     ]
    }
   ],
   "source": [
    "#test code\n",
    "#get left eye location pixel location in real image \n",
    "#shape of real input image\n",
    "inputShape = [480,640]\n",
    "#get cordinates from key Points and multiply it by real image pixel size\n",
    "leftEyelocation = leftEye[:2]*inputShape\n",
    "print(leftEyelocation)\n",
    "#typecast array to numpy array and convert to int data\n",
    "leftEyelocation = np.array(leftEyelocation).astype(int)\n",
    "print(leftEyelocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c613a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to drow keypoints on the image\n",
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 5, (0,255,0), -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdab80c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of conncetions bitween keypoints\n",
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46a568a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "(337, 361)\n",
      "(367, 345)\n",
      "0 2\n",
      "(337, 361)\n",
      "(307, 343)\n",
      "1 3\n",
      "(367, 345)\n",
      "(402, 354)\n",
      "2 4\n",
      "(307, 343)\n",
      "(272, 347)\n",
      "0 5\n",
      "(337, 361)\n",
      "(437, 401)\n",
      "0 6\n",
      "(337, 361)\n",
      "(250, 401)\n",
      "5 7\n",
      "(437, 401)\n",
      "(469, 399)\n",
      "7 9\n",
      "(469, 399)\n",
      "(393, 357)\n",
      "6 8\n",
      "(250, 401)\n",
      "(239, 327)\n",
      "8 10\n",
      "(239, 327)\n",
      "(276, 353)\n",
      "5 6\n",
      "(437, 401)\n",
      "(250, 401)\n",
      "5 11\n",
      "(437, 401)\n",
      "(437, 337)\n",
      "6 12\n",
      "(250, 401)\n",
      "(255, 315)\n",
      "11 12\n",
      "(437, 337)\n",
      "(255, 315)\n",
      "11 13\n",
      "(437, 337)\n",
      "(454, 325)\n",
      "13 15\n",
      "(454, 325)\n",
      "(407, 349)\n",
      "12 14\n",
      "(255, 315)\n",
      "(252, 335)\n",
      "14 16\n",
      "(252, 335)\n",
      "(293, 348)\n"
     ]
    }
   ],
   "source": [
    "#test code\n",
    "#get connected keypoints from dictionary\n",
    "for edge, color in EDGES.items():\n",
    "    p1, p2 = edge\n",
    "    print(p1,p2)\n",
    "    #get corddinates for keypoints\n",
    "    y1, x1, c1 = shaped[p1]\n",
    "    y2, x2, c2 = shaped[p2]\n",
    "    print((int(x1), int(y1)))\n",
    "    print((int(x2), int(y2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35331beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(0,0,255) , BGR value for line color\n",
    "#2 is line thickness\n",
    "\n",
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "870eb841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([361.77632332, 337.40077972,   0.39369392]),\n",
       " array([345.46394348, 367.19223022,   0.56551492]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test code\n",
    "shaped[0],shaped[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8cf7cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.61776323e+02 3.37400780e+02 3.93693924e-01]\n",
      " [3.45463943e+02 3.67192230e+02 5.65514922e-01]\n",
      " [3.43454704e+02 3.07736816e+02 6.12110496e-01]\n",
      " [3.54005127e+02 4.02815323e+02 5.13634861e-01]\n",
      " [3.47404747e+02 2.72240105e+02 5.14383435e-01]\n",
      " [4.01021519e+02 4.37060318e+02 2.48147637e-01]\n",
      " [4.01897278e+02 2.50685558e+02 2.05370575e-01]\n",
      " [3.99602222e+02 4.69801254e+02 7.61049539e-02]\n",
      " [3.27535830e+02 2.39099178e+02 5.34592606e-02]\n",
      " [3.57859011e+02 3.93548126e+02 1.06411144e-01]\n",
      " [3.53690643e+02 2.76599331e+02 5.85712940e-02]\n",
      " [3.37572641e+02 4.37647667e+02 6.07992634e-02]\n",
      " [3.15543938e+02 2.55938931e+02 7.50146359e-02]\n",
      " [3.25084162e+02 4.54429550e+02 6.19133897e-02]\n",
      " [3.35602055e+02 2.52110519e+02 2.95378342e-02]\n",
      " [3.49859962e+02 4.07769623e+02 4.57924828e-02]\n",
      " [3.48638420e+02 2.93986740e+02 3.35203111e-02]]\n",
      "361 337 0.3936939239501953\n",
      "345 367 0.5655149221420288\n",
      "343 307 0.6121104955673218\n",
      "354 402 0.5136348605155945\n",
      "347 272 0.5143834352493286\n",
      "401 437 0.2481476366519928\n",
      "401 250 0.20537057518959045\n",
      "399 469 0.07610495388507843\n",
      "327 239 0.05345926061272621\n",
      "357 393 0.10641114413738251\n",
      "353 276 0.05857129395008087\n",
      "337 437 0.060799263417720795\n",
      "315 255 0.0750146359205246\n",
      "325 454 0.0619133897125721\n",
      "335 252 0.029537834227085114\n",
      "349 407 0.045792482793331146\n",
      "348 293 0.03352031111717224\n"
     ]
    }
   ],
   "source": [
    "#test code\n",
    "#reshapeing ouput to match input image size\n",
    "shaped = np.squeeze(np.multiply(interpreter.get_tensor(interpreter.get_output_details()[0]['index']), [480,640,1]))\n",
    "print(shaped)\n",
    "\n",
    "#loop thrue array and print cordinates\n",
    "for kp in shaped:\n",
    "    ky, kx, kp_conf = kp\n",
    "    print(int(ky), int(kx),kp_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b9089fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#capture webcam for detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    #reshape image to 192 x 192 x 3\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 192,192)\n",
    "    input_image = tf.cast(img, dtype=tf.float32)\n",
    "    \n",
    "    # Setup input and output \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Make predictions \n",
    "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(keypoints_with_scores)\n",
    "    \n",
    "    # Rendering \n",
    "    draw_connections(frame, keypoints_with_scores, EDGES, 0.4)\n",
    "    draw_keypoints(frame, keypoints_with_scores, 0.4)\n",
    "    \n",
    "    cv2.imshow('Movement', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44397ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
